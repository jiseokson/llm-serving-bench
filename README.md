## ì‹¤í—˜ ëª©ì 

- ë‹¤ì–‘í•œ ì„œë¹™ í”„ë ˆì„ì›Œí¬(Hugging Face ğŸ¤— Transformers, TGI, vLLM)ì˜ êµ¬ì¡°ì  ì°¨ì´ ë° ì„±ëŠ¥ì„ ë¹„êµ
- ê°ê¸° ë‹¤ë¥¸ LLM í™œìš© íƒœìŠ¤í¬ì— ëŒ€í•´ latency, throughput, memory usage ë“±ì„ ì¸¡ì •í•˜ê³  ë¶„ì„

## ì‹¤í—˜ êµ¬ì„± ìš”ì†Œ

### ëª¨ë¸

| ëª¨ë¸ ì´ë¦„ | Hugging Face ëª¨ë¸ ID | íŒŒë¼ë¯¸í„° ìˆ˜ | Transformers | TGI | vLLM |
| --- | --- | --- | --- | --- | --- |
| **GPT-2 Small** | `openai-community/gpt2` | 124M | âœ… | âœ… | âœ… |
| **Phi-2** | `microsoft/phi-2` | 2.7B | âœ… | âœ… | âœ… |
| **LLaMA 2 7B** | `meta-llama/Llama-2-7b-chat-hf` | 6.7B | âœ… | âœ… | âœ… |
| **Mistral 7B Instruct** | `mistralai/Mistral-7B-Instruct-v0.1` | 7.3B | âœ… | âœ… | âœ… |

### ì„œë¹™ í”„ë ˆì„ì›Œí¬
| í•­ëª©               | ğŸ¤— Transformers                         | TGI (Text Generation Inference)               | vLLM                                                 |
| ---------------- | --------------------------------------- | --------------------------------------------- | ---------------------------------------------------- |
| **ê°œë°œ ì£¼ì²´**        | Hugging Face                            | Hugging Face                                  | UC Berkeley / LMSys                                  |
| **ì„¤ì¹˜ ë°©ì‹**        | Python íŒ¨í‚¤ì§€<br>(`transformers`, `torch`) | Docker / CLI ê¸°ë°˜<br>(`text-generation-server`) | CLI ê¸°ë°˜ (`vllm`)<br>+ CUDA ì„¤ì • í•„ìš”                      |
| **ì„œë¹™ êµ¬ì¡°**        | ë‹¨ì¼ ìš”ì²­ ì¤‘ì‹¬<br>Python í•¨ìˆ˜ ê¸°ë°˜ ì¶”ë¡              | FastAPI ê¸°ë°˜ REST ì„œë²„<br>ë©€í‹° ìš”ì²­ ë° Streaming ì§€ì›    | C++ ì»¤ë„ + Python ì„œë²„<br>PagedAttention ê¸°ë°˜ ë¹ ë¥¸ Streaming |
| **Streaming ì§€ì›** | âŒ ë¯¸ì§€ì›<br>FastAPI ë“±ìœ¼ë¡œ ì»¤ìŠ¤í…€ í•„ìš”        | âœ… SSE ê¸°ë°˜ Streaming ì‘ë‹µ ì§€ì›                      | âœ… ê³ ì„±ëŠ¥ Streaming ìµœì í™” ë‚´ì¥                               |
| **Batching ì§€ì›**  | âŒ ë¯¸ì§€ì›                              | âœ… ìë™ ë™ì  ë°°ì¹˜<br>(Dynamic batching)              | âœ… Token-level batching<br>+ Prefill/Decode ë³‘ë ¬í™”       |
| **ìµœì í™” íŠ¹ì§•**       | í‘œì¤€ PyTorch ì‹¤í–‰                           | DeepSpeed ê¸°ë°˜ ì˜µí‹°ë§ˆì´ì € ë‚´ì¥                         | PagedAttention ì‚¬ìš©<br>í° context ê¸¸ì´ íš¨ìœ¨ì  ì²˜ë¦¬             |
| **ì í•©í•œ ìš©ë„**       | ì—°êµ¬ ë° í”„ë¡œí† íƒ€ì´í•‘                             | ì‹¤ì„œë¹„ìŠ¤/í”„ë¡œë•ì…˜ í™˜ê²½                                  | ëŒ€ìš©ëŸ‰, ê³ ì„±ëŠ¥ ì„œë¹™                                          |
| **ì¥ì **           | ì‚¬ìš©ë²• ê°„ë‹¨<br>PyTorch ëª¨ë¸ê³¼ ì¼ì²´í™”               | ì•ˆì •ì  API ì„œë²„<br>Hugging Face Ecosystem í†µí•©       | ê³ ì† ì¶”ë¡  + ë‚®ì€ latency<br>Context scaling ìš°ìˆ˜             |
| **ë‹¨ì **           | ì„±ëŠ¥ ìµœì í™” í•œê³„                               | ì´ˆê¸° ì…‹ì—… ë³µì¡<br>(Docker ë“±)                        | ì‹¤í—˜ì  ê¸°ëŠ¥ í¬í•¨<br>ì„¤ì • í•™ìŠµ í•„ìš”                                |

## ì‹¤í—˜ í™˜ê²½

- RunPod A100 (40GB)
- CUDA 11.8.0
- Python 3.10
- PyTorch 2.1.0

## ì‹¤í—˜ ëŒ€ìƒ íƒœìŠ¤í¬

| íƒœìŠ¤í¬ ìœ í˜• | ì…ë ¥ ê¸¸ì´ | ì¶œë ¥ ê¸¸ì´ | ì˜ˆì‹œ |
| --- | --- | --- | --- |
| Chat | 20~100 tokens | 20~100 tokens | ììœ  ì§ˆì˜ì‘ë‹µ |
| Summarization | 200~800 tokens | 50~100 tokens | ë¬¸ì„œ ìš”ì•½ |
| QA (closed-book) | 100~500 tokens | 10~30 tokens | ë‹¨ë‹µí˜• ì§ˆë¬¸ |
| Code Generation | 10~50 tokens | 50~150 tokens | í•¨ìˆ˜ ìë™ ìƒì„± |

## ìƒì„± ìš”ì²­ íŒŒë¼ë¯¸í„°

### GPT-2 Small
| **íƒœìŠ¤í¬** | **temperature** | **top_p** | **max_tokens** |
| --- | --- | --- | --- | 
| Chat | 0.9 | 0.95 | 100 | 
| Summarization | 0.5 | 0.9 | 60 | 
| QA | 0.3 | 1.0 | 40 | 
| Code generation | 0.8 | 0.9 | 100 | 

### Phi-2
| **íƒœìŠ¤í¬** | **temperature** | **top_p** | **max_tokens** | 
| --- | --- | --- | --- | 
| Chat | 0.8 | 0.9 | 100 | 
| Summarization | 0.4 | 0.9 | 60 | 
| QA | 0.2 | 1.0 | 40 | 
| Code generation | 0.7 | 0.9 | 90 | 

### LLaMA 2 7B Chat
| **íƒœìŠ¤í¬** | **temperature** | **top_p** | **max_tokens** | 
| --- | --- | --- | --- | 
| Chat | 0.7 | 0.95 | 150 | 
| Summarization | 0.3 | 0.9 | 80 | 
| QA | 0.1 | 1.0 | 50 | 
| Code generation | 0.6 | 0.85 | 120 | 

### Mistral 7B Instruct
| **íƒœìŠ¤í¬** | **temperature** | **top_p** | **max_tokens** | 
| --- | --- | --- | --- | 
| Chat | 0.8 | 0.95 | 120 | 
| Summarization | 0.3 | 0.9 | 70 | 
| QA | 0.2 | 1.0 | 50 | 
| Code generation | 0.7 | 0.9 | 110 | 

## ì¸¡ì • ì§€í‘œ ë° ìˆ˜ì§‘ ë°©ì‹

| ì§€í‘œ | Sync ë°©ì‹ | Streaming ë°©ì‹ | ë¹„ê³  / ì¸¡ì • ë°©ë²• ìš”ì•½ |
| --- | --- | --- | --- |
| **Total Latency** | âœ… ì „ì²´ ì‘ë‹µ ë„ì°©ê¹Œì§€ ì‹œê°„ ì¸¡ì • | âœ… ë§ˆì§€ë§‰ í† í° ìˆ˜ì‹ ê¹Œì§€ ì‹œê°„ ì¸¡ì • | start_time ~ end_time ì¸¡ì • |
| **Prompt-to-First-Token (P2FT)** | âŒ ì¸¡ì • ì–´ë ¤ì›€, í† í° ë‹¨ìœ„ ìˆ˜ì‹ ì´ ì•„ë‹˜ | âœ… ì²« í† í° ë„ì°© ì‹œê°„ ì¸¡ì • | start_time ~ ì²« SSE line ìˆ˜ì‹  ì‹œì  |
| **Token Generation Speed** (tok/s) | âŒ ì¸¡ì • ì–´ë ¤ì›€, í† í° ë‹¨ìœ„ ìˆ˜ì‹ ì´ ì•„ë‹˜ | âœ… í† í° ìˆ˜ / ì‘ë‹µ ì‹œê°„ | í† í° ìˆ˜ ì¹´ìš´íŒ… & íƒ€ì´ë° í•„ìš” |
| **Throughput** (req/s) | âœ… ë©€í‹° ìš”ì²­ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • | âœ… ë©€í‹° ìš”ì²­ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • | ë³‘ë ¬ ì²˜ë¦¬: asyncio.gather or threading |
| **GPU Memory Usage** | âœ… ë™ì¼ | âœ… ë™ì¼ | pynvml or nvidia-smi |
| **Host Memory Usage** | âœ… ë™ì¼ | âœ… ë™ì¼ | psutil |
| **Context Length Scaling** | âœ… ê°€ëŠ¥ | âœ… ê°€ëŠ¥ | í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë³„ latency ë¹„êµ |
