## ì‹¤í—˜ ëª©ì 

- ë‹¤ì–‘í•œ ì„œë¹™ í”„ë ˆì„ì›Œí¬(Hugging Face ğŸ¤— Transformers, TGI, vLLM)ì˜ êµ¬ì¡°ì  ì°¨ì´ ë° ì„±ëŠ¥ì„ ë¹„êµ
- ê°ê¸° ë‹¤ë¥¸ LLM í™œìš© íƒœìŠ¤í¬ì— ëŒ€í•´ latency, throughput, memory usage ë“±ì„ ì¸¡ì •í•˜ê³  ë¶„ì„

## ì‹¤í—˜ êµ¬ì„± ìš”ì†Œ

### ëª¨ë¸

| ëª¨ë¸ ì´ë¦„ | Hugging Face ëª¨ë¸ ID | íŒŒë¼ë¯¸í„° ìˆ˜ | Transformers | TGI | vLLM |
| --- | --- | --- | --- | --- | --- |
| **GPT-2 Small** | `gpt2` | 124M | âœ… | âœ… | âœ… |
| **Phi-2** | `microsoft/phi-2` | 2.7B | âœ… | âœ… | âœ… |
| **LLaMA 2 7B** | `meta-llama/Llama-2-7b-chat-hf` | 6.7B | âœ… | âœ… | âœ… |
| **Mistral 7B Instruct** | `mistralai/Mistral-7B-Instruct-v0.1` | 7.3B | âœ… | âœ… | âœ… |

## ì„œë¹™ í”„ë ˆì„ì›Œí¬

- Hugging Face ğŸ¤—Â Transformers
- TGI (Text Generation Inference)
- vLLM

## í™˜ê²½

RunPod A100 (40GB)

Python 3.X, CUDA X.X, PyTorch 2.X, â€¦

## ì‹¤í—˜ ëŒ€ìƒ íƒœìŠ¤í¬

| íƒœìŠ¤í¬ ìœ í˜• | ì…ë ¥ ê¸¸ì´ | ì¶œë ¥ ê¸¸ì´ | ì˜ˆì‹œ |
| --- | --- | --- | --- |
| Chat | 20~100 tokens | 20~100 tokens | ììœ  ì§ˆì˜ì‘ë‹µ |
| Summarization | 200~800 tokens | 50~100 tokens | ë¬¸ì„œ ìš”ì•½ |
| QA (closed-book) | 100~500 tokens | 10~30 tokens | ë‹¨ë‹µí˜• ì§ˆë¬¸ |
| Code Generation | 10~50 tokens | 50~150 tokens | í•¨ìˆ˜ ìë™ ìƒì„± |

## ì¸¡ì • ì§€í‘œ ë° ìˆ˜ì§‘ ë°©ì‹

| ì§€í‘œ | ì¸¡ì • ë°©ë²• | ë¹„ê³  |
| --- | --- | --- |
| **Latency (ms)** | ìš”ì²­ ì‹œì‘~ì‘ë‹µ ì™„ë£Œ ì‹œê°„ | í‰ê· , í‘œì¤€í¸ì°¨, ì¤‘ì•™ê°’ ë“± í¬í•¨ |
| **Throughput (req/sec)** | ì´ˆë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìš”ì²­ ìˆ˜ | ë™ì‹œ ìš”ì²­ ì‹¤í—˜ í¬í•¨ |
| **Memory usage (MB)** | `nvidia-smi`, `torch.cuda.memory_allocated()` | peak ë° steady state |
| **Token generation rate** | `tokens/sec` ê¸°ì¤€ | íŠ¹íˆ generation íƒœìŠ¤í¬ì—ì„œ ì¤‘ìš” |
| **Stability under load** | locust or custom async client | ë™ì‹œ ìš”ì²­ 1~20ëª… ì‹œ ì‹¤í—˜ |
